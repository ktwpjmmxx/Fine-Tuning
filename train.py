import json
import os
import torch
from unsloth import FastLanguageModel
from trl import SFTTrainer
from transformers import TrainingArguments
from datasets import load_dataset

# 1. è¨­å®šã®èª­ã¿è¾¼ã¿
with open("config.json", "r") as f:
    config = json.load(f)

# 2. ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ­ãƒ¼ãƒ‰ (Unsloth)
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=config["model_name"],
    max_seq_length=config["max_seq_length"],
    dtype=None,
    load_in_4bit=config["load_in_4bit"],
)

# 3. LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®è¨­å®š
model = FastLanguageModel.get_peft_model(
    model,
    r=config["lora_r"],
    target_modules=config["target_modules"],
    lora_alpha=config["lora_alpha"],
    lora_dropout=config["lora_dropout"],
    bias="none",
    use_gradient_checkpointing="unsloth",
    random_state=3407,
)

# 4. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™ (Promptã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ)
# Elyzaã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå½¢å¼: <s>[INST] {instruction} {input} [/INST] {output} </s>
alpaca_prompt = """<s>[INST] {instruction}
{input} [/INST]
{output} </s>"""

def formatting_prompts_func(examples):
    instructions = examples["instruction"]
    inputs       = examples["input"]
    outputs      = examples["output"]
    texts = []
    for instruction, input, output in zip(instructions, inputs, outputs):
        text = alpaca_prompt.format(instruction=instruction, input=input, output=output)
        texts.append(text)
    return {"text": texts}

dataset = load_dataset("json", data_files=config["dataset_path"], split="train")
dataset = dataset.map(formatting_prompts_func, batched=True)

# 5. å†é–‹(Resume)ã®åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯
# outputsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã«checkpointãƒ•ã‚©ãƒ«ãƒ€ãŒã‚ã‚‹ã‹æ¢ã™
checkpoints = [d for d in os.listdir(config["output_dir"]) if d.startswith("checkpoint-")]
if checkpoints:
    # æ•°å­—éƒ¨åˆ†ã‚’å–ã‚Šå‡ºã—ã¦ã‚½ãƒ¼ãƒˆã—ã€æœ€æ–°ã®ã‚‚ã®ã‚’ç‰¹å®š
    checkpoints.sort(key=lambda x: int(x.split('-')[1]))
    latest_checkpoint = os.path.join(config["output_dir"], checkpoints[-1])
    print(f"ğŸ”„ å‰å›ã®ç¶šãã‹ã‚‰å†é–‹ã—ã¾ã™: {latest_checkpoint}")
    resume_from_checkpoint = latest_checkpoint
else:
    print("æ–°è¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã—ã¾ã™")
    resume_from_checkpoint = False

# 6. ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã®è¨­å®š
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset,
    dataset_text_field="text",
    max_seq_length=config["max_seq_length"],
    dataset_num_proc=2,
    packing=False,
    args=TrainingArguments(
        per_device_train_batch_size=config["per_device_train_batch_size"],
        gradient_accumulation_steps=config["gradient_accumulation_steps"],
        warmup_steps=5,
        num_train_epochs=config["num_train_epochs"],
        learning_rate=config["learning_rate"],
        fp16=not torch.cuda.is_bf16_supported(),
        bf16=torch.cuda.is_bf16_supported(),
        logging_steps=config["logging_steps"],
        optim="adamw_8bit",
        weight_decay=0.01,
        lr_scheduler_type="linear",
        seed=3407,
        output_dir=config["output_dir"],
        save_strategy="steps",      # ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«ä¿å­˜
        save_steps=config["save_steps"],       # configã§æŒ‡å®šã—ãŸé »åº¦
        save_total_limit=2,         # Driveå®¹é‡åœ§è¿«ã‚’é˜²ããŸã‚ã€æœ€æ–°2ã¤ã ã‘æ®‹ã™
    ),
)

# 7. å­¦ç¿’å®Ÿè¡Œ
trainer_stats = trainer.train(resume_from_checkpoint=resume_from_checkpoint)

# 8. æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
print("ğŸ’¾ å­¦ç¿’å®Œäº†ã€‚ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã™...")
model.save_pretrained(os.path.join(config["output_dir"], config["new_model_name"]))
tokenizer.save_pretrained(os.path.join(config["output_dir"], config["new_model_name"]))
print("âœ… ã™ã¹ã¦å®Œäº†ã—ã¾ã—ãŸï¼")