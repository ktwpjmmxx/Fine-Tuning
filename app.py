import streamlit as st
import logging
import torch
from unsloth import FastLanguageModel

# --- 1. åˆæœŸè¨­å®š & è­¦å‘ŠæŠ‘åˆ¶ ---
# ELYZA(Llama-2)ãƒ¢ãƒ‡ãƒ«ç‰¹æœ‰ã®è­¦å‘Šã‚’ç„¡è¦–ã—ã€Unslothã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ã‚’å›é¿ã™ã‚‹è¨­å®š
logging.getLogger("transformers.modeling_utils").setLevel(logging.ERROR)

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="Guardian AI v1",
    page_icon="ğŸ›¡ï¸",
    layout="wide"
)

# --- 2. ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ (ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŒ–) ---
# @st.cache_resource ã‚’ä½¿ã†ã“ã¨ã§ã€ãƒ–ãƒ©ã‚¦ã‚¶ã‚’ãƒªãƒ­ãƒ¼ãƒ‰ã—ã¦ã‚‚ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ç›´ã•ãªã„ï¼ˆçˆ†é€ŸåŒ–ï¼‰
@st.cache_resource
def load_model():
    # å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹ï¼ˆãƒªãƒã‚¸ãƒˆãƒªç›´ä¸‹ã® 'lora_model' ã‚’å‚ç…§ï¼‰
    model_name = "lora_model" 
    
    # ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name = model_name,
        max_seq_length = 2048,
        dtype = None,
        load_in_4bit = True,
    )
    FastLanguageModel.for_inference(model)
    return model, tokenizer

# --- 3. ã‚µã‚¤ãƒ‰ãƒãƒ¼ (è¨­å®šã‚¨ãƒªã‚¢) ---
with st.sidebar:
    st.header("âš™ï¸ ã‚·ã‚¹ãƒ†ãƒ è¨­å®š / Settings")
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰çŠ¶æ…‹ã®è¡¨ç¤º
    with st.status("AIãƒ¢ãƒ‡ãƒ«èµ·å‹•ä¸­...", expanded=True) as status:
        try:
            model, tokenizer = load_model()
            status.update(label="ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº† (Ready)", state="complete", expanded=False)
        except Exception as e:
            status.update(label="ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼", state="error")
            st.error(f"ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}")
    
    st.divider()
    
    # æ¨è«–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´
    st.subheader("æ¨è«–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")
    temperature = st.slider(
        "å³å¯†ã• (Temperature)", 
        min_value=0.0, 
        max_value=1.0, 
        value=0.1, 
        step=0.1,
        help="0ã«è¿‘ã„ã»ã©è«–ç†çš„ã§æ¯å›åŒã˜å›ç­”ã‚’ã—ã¾ã™ã€‚ä¸Šã’ã‚‹ã¨å‰µé€ çš„ã«ãªã‚Šã¾ã™ãŒã€æ³•å‹™ãƒã‚§ãƒƒã‚¯ã§ã¯0.1ã€œ0.3ãŒæ¨å¥¨ã§ã™ã€‚"
    )
    max_tokens = st.slider(
        "å›ç­”ã®é•·ã• (Max Tokens)", 
        min_value=128, 
        max_value=1024, 
        value=512,
        step=64
    )
    
    st.markdown("---")
    st.caption("Developed by Guardian AI Project")
    st.caption("Base Model: ELYZA-japanese-Llama-2-7b")

# --- 4. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå®šç¾© (Alpaca Format) ---
alpaca_prompt = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{}

### Input:
{}

### Response:
{}"""

# --- 5. ãƒ¡ã‚¤ãƒ³UI ---
st.title("ğŸ›¡ï¸ Guardian AI v1")
st.markdown("### ITæ³•å‹™ç‰¹åŒ–å‹ ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹ãƒã‚§ãƒƒã‚¯AI")
st.info("å¥‘ç´„æ›¸ã®æ¡æ–‡ã€ä»•æ§˜æ›¸ã®ãƒ†ã‚­ã‚¹ãƒˆã€ã¾ãŸã¯æ³•çš„ãªç›¸è«‡å†…å®¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚AIãŒãƒªã‚¹ã‚¯åˆ¤å®šã¨ä¿®æ­£æ¡ˆã‚’æç¤ºã—ã¾ã™ã€‚")

# ç”»é¢ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ (2ã‚«ãƒ©ãƒ )
col1, col2 = st.columns([1, 1])

# å·¦å´ï¼šå…¥åŠ›ã‚¨ãƒªã‚¢
with col1:
    st.subheader("ğŸ“ å…¥åŠ› (Input)")
    input_text = st.text_area(
        "è§£æå¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆ",
        height=400,
        placeholder="ã“ã“ã«å¥‘ç´„æ›¸ã®æ¡æ–‡ã‚„ç›¸è«‡å†…å®¹ã‚’è²¼ã‚Šä»˜ã‘ã¦ãã ã•ã„ã€‚\n\nä¾‹ï¼š\næå®³è³ å„Ÿã®è«‹æ±‚é¡ã¯ã€ç†ç”±ã®å¦‚ä½•ã‚’å•ã‚ãšã€æœ¬å¥‘ç´„ã«åŸºã¥ãç”²ãŒä¹™ã«æ”¯æ‰•ã£ãŸç›´è¿‘1ãƒ¶æœˆåˆ†ã®å§”è¨—æ–™ã‚’ä¸Šé™ã¨ã™ã‚‹ã€‚"
    )
    
    analyze_btn = st.button("ãƒªã‚¹ã‚¯åˆ¤å®šã‚’å®Ÿè¡Œ (Analyze)", type="primary", use_container_width=True)

# å³å´ï¼šå‡ºåŠ›ã‚¨ãƒªã‚¢
with col2:
    st.subheader("âš–ï¸ è¨ºæ–­çµæœ (Result)")
    
    if analyze_btn:
        if not input_text:
            st.warning("âš ï¸ ãƒ†ã‚­ã‚¹ãƒˆãŒå…¥åŠ›ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        else:
            with st.spinner("æ¡é …ã‚’è§£æä¸­... (AIãŒæ€è€ƒã—ã¦ã„ã¾ã™)"):
                try:
                    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ§‹ç¯‰
                    prompt = alpaca_prompt.format(
                        "ITæ³•å‹™ã®å°‚é–€å®¶ã¨ã—ã¦ã€ä»¥ä¸‹ã®æ¡é …ã®ãƒªã‚¹ã‚¯ã‚’åˆ¤å®šã—ã€ä¿®æ­£æ¡ˆã‚’æç¤ºã—ã¦ãã ã•ã„ã€‚", 
                        input_text, 
                        ""
                    )
                    
                    # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã¨GPUã¸ã®è»¢é€
                    inputs = tokenizer([prompt], return_tensors="pt").to("cuda")

                    # æ¨è«–å®Ÿè¡Œ
                    outputs = model.generate(
                        **inputs, 
                        max_new_tokens = max_tokens,
                        use_cache = True,
                        temperature = temperature, 
                    )
                    
                    # çµæœã®ãƒ‡ã‚³ãƒ¼ãƒ‰ã¨æ•´å½¢
                    response_text = tokenizer.batch_decode(outputs)[0]
                    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã‚’é™¤å»ã—ã¦å›ç­”éƒ¨åˆ†ã ã‘æŠ½å‡º
                    cleaned_response = response_text.split("### Response:\n")[-1].replace(tokenizer.eos_token, "")
                    
                    # çµæœè¡¨ç¤º
                    st.success("è§£æå®Œäº†")
                    st.markdown(cleaned_response)
                    
                except Exception as e:
                    st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

    else:
        st.info("ğŸ‘ˆ å·¦å´ã®ãƒ•ã‚©ãƒ¼ãƒ ã«å…¥åŠ›ã—ã¦ãƒœã‚¿ãƒ³ã‚’æŠ¼ã™ã¨ã€ã“ã“ã«çµæœãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚")

# ãƒ•ãƒƒã‚¿ãƒ¼
st.markdown("---")
st.markdown(
    """
    <div style='text-align: center; color: gray; font-size: 0.8em;'>
    â€»æœ¬AIã®å›ç­”ã¯å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãäºˆæ¸¬ã§ã‚ã‚Šã€æ³•çš„åŠ©è¨€ï¼ˆãƒªãƒ¼ã‚¬ãƒ«ã‚¢ãƒ‰ãƒã‚¤ã‚¹ï¼‰ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚<br>
    æœ€çµ‚çš„ãªå¥‘ç´„åˆ¤æ–­ã‚„ç´›äº‰è§£æ±ºã«ã‚ãŸã£ã¦ã¯ã€å¿…ãšå¼è­·å£«ç­‰ã®å°‚é–€å®¶ã«ã”ç›¸è«‡ãã ã•ã„ã€‚
    </div>
    """, 
    unsafe_allow_html=True
)