import json
import torch
from unsloth import FastLanguageModel

# 1. è¨­å®šã®èª­ã¿è¾¼ã¿
with open("config.json", "r") as f:
    config = json.load(f)

# å­¦ç¿’æ¸ˆã¿ã®ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ï¼ˆoutputsãƒ•ã‚©ãƒ«ãƒ€ã®ä¸­ã®ãƒ¢ãƒ‡ãƒ«åã‚’æŒ‡å®šï¼‰
model_path = os.path.join(config["output_dir"], config["new_model_name"])

print(f"ğŸ“‚ ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­: {model_path}")

# 2. ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=model_path, # ã“ã“ã§å­¦ç¿’æ¸ˆã¿ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’æŒ‡å®š
    max_seq_length=config["max_seq_length"],
    dtype=None,
    load_in_4bit=True,
)
FastLanguageModel.for_inference(model)

# 3. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
alpaca_prompt = """<s>[INST] {instruction}
{input} [/INST]
"""

# 4. æ¨è«–å®Ÿè¡Œé–¢æ•°
def generate_response(instruction, input_text):
    prompt = alpaca_prompt.format(instruction=instruction, input=input_text)
    inputs = tokenizer([prompt], return_tensors="pt").to("cuda")

    outputs = model.generate(
        **inputs,
        max_new_tokens=512,
        use_cache=True,
        temperature=0.3, # æ³•å‹™ãªã®ã§å°‘ã—ä½ã‚ï¼ˆãƒ©ãƒ³ãƒ€ãƒ æ€§ã‚’æŠ‘ãˆã‚‹ï¼‰
        top_p=0.9,
    )
    result = tokenizer.batch_decode(outputs)
    # ç”Ÿæˆéƒ¨åˆ†ã®ã¿æŠ½å‡ºã™ã‚‹å‡¦ç†ï¼ˆç°¡æ˜“ç‰ˆï¼‰
    return result[0].split("[/INST]")[1].replace("</s>", "").strip()

# --- ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ ---
if __name__ == "__main__":
    test_instruction = "ITæ³•å‹™ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆã¨ã—ã¦ã€ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆä»•æ§˜ã«é–¢é€£ã™ã‚‹æ³•çš„ãƒªã‚¹ã‚¯ã‚’åˆ¤å®šã—ã€å®Ÿå‹™çš„ãªä¿®æ­£æ¡ˆã‚’æç¤ºã—ã¦ãã ã•ã„ã€‚"
    test_input = "é€€ä¼šãƒœã‚¿ãƒ³ã‚’ã‚ãˆã¦è¦‹ã¤ã‘ã«ãã„å ´æ‰€ã«é…ç½®ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®é›¢è„±ã‚’é˜²ããŸã„ã§ã™ã€‚"
    
    print("\n--- Input ---")
    print(test_input)
    print("\n--- Output ---")
    print(generate_response(test_instruction, test_input))