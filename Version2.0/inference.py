from unsloth import FastLanguageModel
import torch
import os

# --- è¨­å®š ---
# å­¦ç¿’æ™‚ã«ä¿å­˜ã—ãŸDriveã®ãƒ‘ã‚¹
MODEL_PATH = "/content/drive/MyDrive/Llama3_FineTune/lora_model_llama3"

# ãƒ†ã‚¹ãƒˆã—ãŸã„å…¥åŠ›
test_instruction = "ITæ³•å‹™ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆã¨ã—ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚"
test_input = "é–‹ç™ºå§”è¨—å¥‘ç´„ã§ã€ç´å“ç‰©ã®æ¤œåæœŸé–“ã‚’ã€æ°¸ç¶šçš„ã«ã€è¨­å®šã—ãŸã„ã¨è¨€ã‚ã‚ŒãŸãŒã€ãƒªã‚¹ã‚¯ã¯ã‚ã‚‹ã‹ï¼Ÿ"

# --- æ¨è«–å®Ÿè¡Œ ---
print(f"ğŸ“‚ Driveã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ã„ã¾ã™: {MODEL_PATH}")

if not os.path.exists(MODEL_PATH):
    print("âŒ ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚train.pyãŒæ­£å¸¸ã«å®Œäº†ã—ã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    exit()

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = MODEL_PATH,
    max_seq_length = 4096,
    dtype = None,
    load_in_4bit = True,
)
FastLanguageModel.for_inference(model)

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
prompt = f"""<|start_header_id|>system<|end_header_id|>

{test_instruction}<|eot_id|><|start_header_id|>user<|end_header_id|>

{test_input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>
"""

inputs = tokenizer([prompt], return_tensors = "pt").to("cuda")

print("ç”Ÿæˆä¸­...")
outputs = model.generate(
    **inputs, 
    max_new_tokens = 512, 
    use_cache = True,
    temperature = 0.1, # äº‹å®Ÿé‡è¦–è¨­å®š
)

result = tokenizer.batch_decode(outputs)
# ä¸è¦ãªç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã‚’é™¤å»ã—ã¦è¡¨ç¤º
clean_output = result[0].split("<|start_header_id|>assistant<|end_header_id|>")[-1].replace("<|eot_id|>", "")

print("\n=== æ¨è«–çµæœ ===")
print(clean_output)
print("==================")